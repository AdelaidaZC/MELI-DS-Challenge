{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adelaidazuluaga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adelaidazuluaga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "import requests\n",
    "import unidecode\n",
    "import os, glob\n",
    "from pathlib import Path  \n",
    "import cv2\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import shutil, sys\n",
    "import urllib.request\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descargar un total de 1000 datos por clase: 80% train, 20% test\n",
    "#OFFSETS_TRAIN = list(range(0,550,50)) #550 datos\n",
    "OFFSETS_LIST= list(range(0,50,50)) #550 datos\n",
    "#OFFSETS_TEST = list(range(550,700,50)) #150 datos\n",
    "#OFFSETS_VAL = list(range(700,850,50)) #150 datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requried Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from opencv-python) (1.23.0)\n",
      "Requirement already satisfied: plotly==5.10.0 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (5.10.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from plotly==5.10.0) (8.0.1)\n",
      "Requirement already satisfied: pillow in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (9.2.0)\n",
      "Requirement already satisfied: regex in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (2022.9.11)\n",
      "Requirement already satisfied: Unidecode in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: wordcloud in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from wordcloud) (3.5.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from wordcloud) (1.23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.37.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: nltk in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from nltk) (2022.9.11)\n",
      "Requirement already satisfied: joblib in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /Users/adelaidazuluaga/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from nltk) (8.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install plotly==5.10.0\n",
    "!pip install pillow\n",
    "!pip install regex\n",
    "!pip install Unidecode\n",
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product categories extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = requests.get('https://api.mercadolibre.com/sites/MLA/categories')\n",
    "prod_categories= cats.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain product sub categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_sub_categories(general_category): \n",
    "    try:\n",
    "        url = f'https://api.mercadolibre.com/sites/MLA/search?category={general_category}'\n",
    "        request = requests.get(url)\n",
    "        items = request.json()\n",
    "        sub_categories_names = [i['name'] for i in items['available_filters'][0]['values']]\n",
    "        sub_categories_ids = [i['id'] for i in items['available_filters'][0]['values']]\n",
    "        df_sub_cat = pd.DataFrame(list(zip(sub_categories_ids, sub_categories_names)),columns=['id','Name'])\n",
    "    except Exception as e:\n",
    "        print (e.message, e.args)\n",
    "    return df_sub_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively download product information for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_json_items_iteratively(df_sub_cat,offset_list):\n",
    "    offsets= list(range(0,50,50)) #MIL ELEMENTOS POR SUBCATEGORIA\n",
    "    results = []\n",
    "    sub_cat_list = []\n",
    "    offsets = offset_list\n",
    "    sub_cat_id = list(df_sub_cat['id'])\n",
    "    sub_cat_name = list(df_sub_cat['Name'])\n",
    "    for c in sub_cat_id:\n",
    "        for off in offsets: \n",
    "            url = f'https://api.mercadolibre.com/sites/MLA/search?category={c}&offset={off}'\n",
    "            request = requests.get(url)\n",
    "            data = request.json()\n",
    "            try:\n",
    "                for element in data['results']:\n",
    "                    results.append(element)\n",
    "                    sub_cat_list.append(list(df_sub_cat[df_sub_cat['id']==c]['Name'])[0])\n",
    "            except: \n",
    "                print(\"ERROR\",data)\n",
    "    return results,sub_cat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract product brand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_brand(items_cat):\n",
    "    brands = []\n",
    "    price = []\n",
    "    for att in items_cat['attributes']:\n",
    "        find_attr = att['id']\n",
    "        if 'BRAND' in find_attr or 'Brand' in find_attr:\n",
    "            brands.append(att['value_name'])\n",
    "            break\n",
    "    return brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning (NLP preprocessing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_nlp(text_df):\n",
    "    cleaned_df =[]\n",
    "    for text in text_df:\n",
    "        clean_text = regex.sub(r'\\&[a-z]+\\;', '', text)\n",
    "        clean_text = regex.sub(r\"\\n\", \" \", text)\n",
    "        clean_text = text.lower()\n",
    "        clean_text = unidecode.unidecode(clean_text)\n",
    "        clean_text = regex.sub(r'\\s+', ' ', clean_text)\n",
    "        clean_text = regex.sub('[^a-zA-Z]', ' ', clean_text).strip(' ')\n",
    "        cleaned_df.append(clean_text)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(text_name):\n",
    "    text = text_name\n",
    "    clean_text = regex.sub(r'\\&[a-z]+\\;', '', text)\n",
    "    clean_text = regex.sub(r\"\\n\", \" \", text)\n",
    "    clean_text = text.lower()\n",
    "    clean_text = unidecode.unidecode(clean_text)\n",
    "    clean_text = regex.sub(r'\\s+', ' ', clean_text)\n",
    "    clean_text = regex.sub('[^a-zA-Z]', ' ', clean_text).strip(' ')\n",
    "    cleaned_df = clean_text\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe for each subset of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pd_product(item_json,item_category, item_sub_cat):\n",
    "    titles_=[]\n",
    "    marcas_=[]\n",
    "    image_=[]\n",
    "    price_=[]\n",
    "    sub_cat_ = []\n",
    "    items = item_json\n",
    "    for i in range(len(items)):\n",
    "        titles_.append(items[i]['title'].lower()) \n",
    "        marcas_.append(get_product_brand(items[i])) #extraer la marca\n",
    "        image_.append(items[i]['thumbnail'])\n",
    "        price_.append(items[i]['price'])\n",
    "        sub_cat_.append(clean_name(item_sub_cat[i]))\n",
    "    df_products = pd.DataFrame(columns=['nombre','categoria','sub_categoria','marca','precio','imagen','nombre_preproc'])\n",
    "    df_products['nombre'] = titles_\n",
    "    df_products['categoria'] = clean_name(item_category)\n",
    "    #df_products['num_categ'] = dict_categs[item_category]*len(items)\n",
    "    df_products['sub_categoria'] = sub_cat_\n",
    "    df_products['marca'] = marcas_\n",
    "    df_products['precio'] = price_\n",
    "    df_products['imagen'] = image_\n",
    "    df_products['nombre_preproc'] = pre_proc_nlp(df_products['nombre'])\n",
    "    return df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_dict_train = {}\n",
    "#category_dict_test = {}\n",
    "#category_dict_val = {}\n",
    "#for category in tqdm(prod_categories): \n",
    "    #print(category)\n",
    "    #obtener diccionario de elementos para train, test y val\n",
    "    #df_cat = obtain_sub_categories(category['id'])\n",
    "    #item_json_train,item_sub_category_train = obtain_json_items_iteratively(df_cat,OFFSETS_TRAIN) #train\n",
    "    #item_json_test,item_sub_category_test = obtain_json_items_iteratively(df_cat,OFFSETS_TEST) #test\n",
    "    #item_json_val,item_sub_category_val = obtain_json_items_iteratively(df_cat,OFFSETS_VAL) #test\n",
    "    #cat_name = category['name']\n",
    "    #train dataset \n",
    "    #df_cat_prod_train = create_pd_product(item_json_train,cat_name,item_sub_category_train)\n",
    "    #category_dict_train[df_cat_prod_train['categoria'][0]] = df_cat_prod_train\n",
    "    #test dataset\n",
    "    #df_cat_prod_test = create_pd_product(item_json_test,cat_name, item_sub_category_test)\n",
    "    #category_dict_test[df_cat_prod_test['categoria'][0]] = df_cat_prod_test\n",
    "    #val dataset\n",
    "    #df_cat_prod_val = create_pd_product(item_json_val,cat_name, item_sub_category_val)\n",
    "    #category_dict_val[df_cat_prod_val['categoria'][0]] = df_cat_prod_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                              | 0/32 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "category_dict = {}\n",
    "for category in tqdm(prod_categories): \n",
    "    #obtener diccionario de elementos para train, test y val\n",
    "    df_cat = obtain_sub_categories(category['id'])\n",
    "    item_json,item_sub_category = obtain_json_items_iteratively(df_cat,OFFSETS_LIST) #train\n",
    "    cat_name = category['name']\n",
    "    df_cat_prod = create_pd_product(item_json,cat_name,item_sub_category)\n",
    "    category_dict[df_cat_prod['categoria'][0]] = df_cat_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.concat([category_dict [cat] for cat in category_dict], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_registers = pd.DataFrame(df_dataset['sub_categoria'].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.767045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72.507921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_categoria\n",
       "count     352.000000\n",
       "mean       54.767045\n",
       "std        72.507921\n",
       "min         2.000000\n",
       "25%        50.000000\n",
       "50%        50.000000\n",
       "75%        50.000000\n",
       "max      1400.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_registers.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se decide tomar únicamenteo 50 registros por subcategoria, debido a la distribución de registros en cada subcategoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar One hot encoding para cada sub categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_train = pd.concat([category_dict_train[cat] for cat in category_dict_train], axis=0)\n",
    "df_dataset_test = pd.concat([category_dict_test[cat] for cat in category_dict_test], axis=0)\n",
    "df_dataset_val = pd.concat([category_dict_val[cat] for cat in category_dict_val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenar los 3 dfs y generar columna de sub clase numerica \n",
    "df_dataset_train['subset'] = 'train'\n",
    "df_dataset_test['subset'] = 'test'\n",
    "df_dataset_val['subset'] = 'val'\n",
    "df_total = pd.concat([df_dataset_train,df_dataset_test,df_dataset_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_categs(categories):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(categories)\n",
    "    return integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_sub_categories = encoding_categs(df_total['sub_categoria'].unique())\n",
    "dict_sub_categs = dict(zip(df_total['sub_categoria'].unique(),numeric_sub_categories))\n",
    "num_list = []\n",
    "for cat_in in df_total['sub_categoria']: \n",
    "    num_list.append(dict_sub_categs[cat_in])\n",
    "df_total['numeric_sub_cat'] = num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total['numeric_sub_cat'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 350 subcategorías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>categoria</th>\n",
       "      <th>sub_categoria</th>\n",
       "      <th>marca</th>\n",
       "      <th>precio</th>\n",
       "      <th>imagen</th>\n",
       "      <th>nombre_preproc</th>\n",
       "      <th>numeric_sub_cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>53389</td>\n",
       "      <td>53389</td>\n",
       "      <td>53389</td>\n",
       "      <td>53389</td>\n",
       "      <td>52553</td>\n",
       "      <td>53389</td>\n",
       "      <td>53389</td>\n",
       "      <td>53389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>202907</td>\n",
       "      <td>202907</td>\n",
       "      <td>202907</td>\n",
       "      <td>202907</td>\n",
       "      <td>198456</td>\n",
       "      <td>202907</td>\n",
       "      <td>202907</td>\n",
       "      <td>202907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>52301</td>\n",
       "      <td>52301</td>\n",
       "      <td>52301</td>\n",
       "      <td>52301</td>\n",
       "      <td>51614</td>\n",
       "      <td>52301</td>\n",
       "      <td>52301</td>\n",
       "      <td>52301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nombre  categoria  sub_categoria   marca  precio  imagen  \\\n",
       "subset                                                             \n",
       "test     53389      53389          53389   53389   52553   53389   \n",
       "train   202907     202907         202907  202907  198456  202907   \n",
       "val      52301      52301          52301   52301   51614   52301   \n",
       "\n",
       "        nombre_preproc  numeric_sub_cat  \n",
       "subset                                   \n",
       "test             53389            53389  \n",
       "train           202907           202907  \n",
       "val              52301            52301  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_counts = df_total.groupby('subset').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total de registros: 308,597\n",
    "* Entrenamiento: 202,907 = 65%\n",
    "* Prueba: 53,389 = 17%\n",
    "* Validación: 52,301 = 16.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "filepath = Path(cwd+'/dataset_train_sub_cats.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_total[df_total['subset']=='train'].to_csv(filepath,index=False)  \n",
    "\n",
    "filepath = Path(cwd+'/dataset_test_sub_cats.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_total[df_total['subset']=='test'].to_csv(filepath,index=False)  \n",
    "\n",
    "filepath = Path(cwd+'/dataset_val_sub_cats.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_total[df_total['subset']=='val'].to_csv(filepath,index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_total\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_total' is not defined"
     ]
    }
   ],
   "source": [
    "df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_route, category,name_folder):\n",
    "    for ind,img_url in enumerate(image_route):\n",
    "        url = img_url\n",
    "        file_name = category + str(ind) +'.jpg' \n",
    "        save_path = './sub_images_'+name_folder+'/'+ category\n",
    "        # Check whether the specified path exists or not\n",
    "        if ind==0:\n",
    "          # Create a new directory because it does not exist \n",
    "          os.makedirs(save_path)\n",
    "        completeName = os.path.join(save_path, file_name)\n",
    "        if len(url)>0:\n",
    "            urllib.request.urlretrieve(url, completeName)\n",
    "        else: \n",
    "            print(ind,category)\n",
    "            print(url)\n",
    "            print(\"problema con url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "filepath = Path(cwd+'/dataset_train_sub_cats.csv')  \n",
    "df_train = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                              | 0/32 [20:35<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download train images\n",
    "for c_ in tqdm(df_train['categoria'].unique()):\n",
    "    download_images(df_train[df_train['categoria']==c_]['imagen'],c_,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11591"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta = df_train[df_train['categoria']==c_]['imagen']\n",
    "len(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for ind,img_url in enumerate(ruta):\n",
    "    print(ind)\n",
    "    url = img_url\n",
    "    file_name = category + str(ind) +'.jpg' \n",
    "    save_path = './sub_images_'+'train'+'/'+ 'test'\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(save_path)\n",
    "    if ind ==0:\n",
    "        os.makedirs(save_path)\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    if len(url)>0:\n",
    "        urllib.request.urlretrieve(url, completeName)\n",
    "    else: \n",
    "        print(ind,category)\n",
    "        print(url)\n",
    "        print(\"problema con url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "filepath = Path(cwd+'/dataset_test_sub_cats.csv')  \n",
    "df_test = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test images\n",
    "for c_ in tqdm(df_test['categoria'].unique()):\n",
    "    download_images(df_test[df_test['categoria']==c_]['imagen'],c_,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "filepath = Path(cwd+'/dataset_val_sub_cats.csv')  \n",
    "df_val = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test images\n",
    "for c_ in tqdm(df_val['categoria'].unique()):\n",
    "    download_images(df_val[df_val['categoria']==c_]['imagen'],c_,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
