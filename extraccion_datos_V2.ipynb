{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adelaidazuluaga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adelaidazuluaga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import requests\n",
    "import unidecode\n",
    "import os, glob\n",
    "from pathlib import Path  \n",
    "import cv2\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import shutil, sys\n",
    "import urllib.request\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSETS_LIST= list(range(0,50,50)) #550 datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requried Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product categories extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = requests.get('https://api.mercadolibre.com/sites/MLA/categories')\n",
    "prod_categories= cats.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain product sub categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_sub_categories(general_category:str): \n",
    "    try:\n",
    "        url = f'https://api.mercadolibre.com/sites/MLA/search?category={general_category}'\n",
    "        request = requests.get(url)\n",
    "        items = request.json()\n",
    "        sub_categories_names = [i['name'] for i in items['available_filters'][0]['values']]\n",
    "        sub_categories_ids = [i['id'] for i in items['available_filters'][0]['values']]\n",
    "        df_sub_cat = pd.DataFrame(list(zip(sub_categories_ids, sub_categories_names)),columns=['id','Name'])\n",
    "    except Exception as e:\n",
    "        print (e.message, e.args)\n",
    "    return df_sub_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively download product information for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_json_items_iteratively(df_sub_cat,offset_list):\n",
    "    offsets= list(range(0,50,50)) #50 ELEMENTOS POR SUBCATEGORIA\n",
    "    results = []\n",
    "    sub_cat_list = []\n",
    "    offsets = offset_list\n",
    "    sub_cat_id = list(df_sub_cat['id'])\n",
    "    sub_cat_name = list(df_sub_cat['Name'])\n",
    "    for c in sub_cat_id:\n",
    "        url = f'https://api.mercadolibre.com/sites/MLA/search?category={c}&offset={offsets[0]}'\n",
    "        request = requests.get(url)\n",
    "        data = request.json()\n",
    "        for element in data['results']:\n",
    "            results.append(element)\n",
    "            sub_cat_list.append(list(df_sub_cat[df_sub_cat['id']==c]['Name'])[0])\n",
    "\n",
    "    return results,sub_cat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract product brand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_brand(items_cat):\n",
    "    brands = []\n",
    "    price = []\n",
    "    for att in items_cat['attributes']:\n",
    "        find_attr = att['id']\n",
    "        if 'BRAND' in find_attr or 'Brand' in find_attr:\n",
    "            brands.append(att['value_name'])\n",
    "            break\n",
    "    return brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning (NLP preprocessing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_nlp(text_df):\n",
    "    cleaned_df =[]\n",
    "    for text in text_df:\n",
    "        clean_text = regex.sub(r'\\&[a-z]+\\;', '', text)\n",
    "        clean_text = regex.sub(r\"\\n\", \" \", text)\n",
    "        clean_text = text.lower()\n",
    "        clean_text = unidecode.unidecode(clean_text)\n",
    "        clean_text = regex.sub(r'\\s+', ' ', clean_text)\n",
    "        clean_text = regex.sub('[^a-zA-Z]', ' ', clean_text).strip(' ')\n",
    "        cleaned_df.append(clean_text)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(text_name):\n",
    "    text = text_name\n",
    "    clean_text = regex.sub(r'\\&[a-z]+\\;', '', text)\n",
    "    clean_text = regex.sub(r\"\\n\", \" \", text)\n",
    "    clean_text = text.lower()\n",
    "    clean_text = unidecode.unidecode(clean_text)\n",
    "    clean_text = regex.sub(r'\\s+', ' ', clean_text)\n",
    "    clean_text = regex.sub('[^a-zA-Z]', ' ', clean_text).strip(' ')\n",
    "    cleaned_df = clean_text\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe for products information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pd_product(item_json,item_category, item_sub_cat):\n",
    "    titles_=[]\n",
    "    marcas_=[]\n",
    "    image_=[]\n",
    "    price_=[]\n",
    "    sub_cat_ = []\n",
    "    items = item_json\n",
    "    for i in range(len(items)):\n",
    "        titles_.append(items[i]['title'].lower()) \n",
    "        marcas_.append(get_product_brand(items[i])) #extraer la marca\n",
    "        image_.append(items[i]['thumbnail'])\n",
    "        price_.append(items[i]['price'])\n",
    "        sub_cat_.append(clean_name(item_sub_cat[i]))\n",
    "    df_products = pd.DataFrame(columns=['nombre','categoria','sub_categoria','marca','precio','imagen','nombre_preproc'])\n",
    "    df_products['nombre'] = titles_\n",
    "    df_products['categoria'] = clean_name(item_category)\n",
    "    #df_products['num_categ'] = dict_categs[item_category]*len(items)\n",
    "    df_products['sub_categoria'] = sub_cat_\n",
    "    df_products['marca'] = marcas_\n",
    "    df_products['precio'] = price_\n",
    "    df_products['imagen'] = image_\n",
    "    df_products['nombre_preproc'] = pre_proc_nlp(df_products['nombre'])\n",
    "    return df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_dict_train = {}\n",
    "#category_dict_test = {}\n",
    "#category_dict_val = {}\n",
    "#for category in tqdm(prod_categories): \n",
    "    #print(category)\n",
    "    #obtener diccionario de elementos para train, test y val\n",
    "    #df_cat = obtain_sub_categories(category['id'])\n",
    "    #item_json_train,item_sub_category_train = obtain_json_items_iteratively(df_cat,OFFSETS_TRAIN) #train\n",
    "    #item_json_test,item_sub_category_test = obtain_json_items_iteratively(df_cat,OFFSETS_TEST) #test\n",
    "    #item_json_val,item_sub_category_val = obtain_json_items_iteratively(df_cat,OFFSETS_VAL) #test\n",
    "    #cat_name = category['name']\n",
    "    #train dataset \n",
    "    #df_cat_prod_train = create_pd_product(item_json_train,cat_name,item_sub_category_train)\n",
    "    #category_dict_train[df_cat_prod_train['categoria'][0]] = df_cat_prod_train\n",
    "    #test dataset\n",
    "    #df_cat_prod_test = create_pd_product(item_json_test,cat_name, item_sub_category_test)\n",
    "    #category_dict_test[df_cat_prod_test['categoria'][0]] = df_cat_prod_test\n",
    "    #val dataset\n",
    "    #df_cat_prod_val = create_pd_product(item_json_val,cat_name, item_sub_category_val)\n",
    "    #category_dict_val[df_cat_prod_val['categoria'][0]] = df_cat_prod_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate category products dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [04:58<00:00,  9.32s/it]\n"
     ]
    }
   ],
   "source": [
    "category_dict = {}\n",
    "for category in tqdm(prod_categories): \n",
    "    df_cat = obtain_sub_categories(category['id'])\n",
    "    item_json,item_sub_category = obtain_json_items_iteratively(df_cat,OFFSETS_LIST) #train\n",
    "    cat_name = category['name']\n",
    "    df_cat_prod = create_pd_product(item_json,cat_name,item_sub_category)\n",
    "    category_dict[df_cat_prod['categoria'][0]] = df_cat_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.concat([category_dict [cat] for cat in category_dict], axis=0)\n",
    "df_cats_sub_cats = pd.DataFrame(df_dataset[['sub_categoria','categoria']].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_categoria</th>\n",
       "      <th>categoria</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aberturas</td>\n",
       "      <td>construccion</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>otros</td>\n",
       "      <td>belleza y cuidado personal</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>otros</td>\n",
       "      <td>herramientas</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otros</td>\n",
       "      <td>electronica  audio y video</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>otros</td>\n",
       "      <td>electrodomesticos y aires ac</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>acido clorhidrico</td>\n",
       "      <td>otras categorias</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>eventos deportivos</td>\n",
       "      <td>entradas para eventos</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>coberturas extendidas</td>\n",
       "      <td>otras categorias</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>eventos a beneficio</td>\n",
       "      <td>entradas para eventos</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>tags para el pago de peajes</td>\n",
       "      <td>accesorios para vehiculos</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sub_categoria                     categoria   0\n",
       "0                      aberturas                  construccion  50\n",
       "1                          otros    belleza y cuidado personal  50\n",
       "2                          otros                  herramientas  50\n",
       "3                          otros    electronica  audio y video  50\n",
       "4                          otros  electrodomesticos y aires ac  50\n",
       "..                           ...                           ...  ..\n",
       "383            acido clorhidrico              otras categorias  33\n",
       "384           eventos deportivos         entradas para eventos  29\n",
       "385        coberturas extendidas              otras categorias  27\n",
       "386          eventos a beneficio         entradas para eventos  15\n",
       "387  tags para el pago de peajes     accesorios para vehiculos   2\n",
       "\n",
       "[388 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cats_sub_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_registers = pd.DataFrame(df_dataset['sub_categoria'].value_counts()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.767045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72.507921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_categoria\n",
       "count     352.000000\n",
       "mean       54.767045\n",
       "std        72.507921\n",
       "min         2.000000\n",
       "25%        50.000000\n",
       "50%        50.000000\n",
       "75%        50.000000\n",
       "max      1400.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_registers.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se decide tomar únicamenteo 50 registros por subcategoria, debido a la distribución de registros en cada subcategoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar One hot encoding para cada sub categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_categs(categories):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(categories)\n",
    "    return integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_sub_categories = encoding_categs(df_dataset['sub_categoria'].unique())\n",
    "dict_sub_categs = dict(zip(df_dataset['sub_categoria'].unique(),numeric_sub_categories))\n",
    "num_list = []\n",
    "for cat_in in df_dataset['sub_categoria']: \n",
    "    num_list.append(dict_sub_categs[cat_in])\n",
    "df_dataset['numeric_sub_cat'] = num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset['numeric_sub_cat'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 350 subcategorías "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "filepath = Path(cwd+'/dataset_sub_cats.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_dataset.to_csv(filepath,index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_route, category,name_folder):\n",
    "    for ind,img_url in enumerate(image_route):\n",
    "        url = img_url\n",
    "        file_name = category + str(ind) +'.jpg' \n",
    "        save_path = './sub_images_'+name_folder+'/'+ category\n",
    "        # Check whether the specified path exists or not\n",
    "        if ind==0:\n",
    "          # Create a new directory because it does not exist \n",
    "          os.makedirs(save_path)\n",
    "        completeName = os.path.join(save_path, file_name)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, completeName)\n",
    "        except: \n",
    "            print(ind,category)\n",
    "            print(url)\n",
    "            print(\"problema con url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████████████████████████████████████▋                                                                             | 17/32 [05:52<06:27, 25.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 entradas para eventos\n",
      "\n",
      "problema con url\n",
      "184 entradas para eventos\n",
      "\n",
      "problema con url\n",
      "205 entradas para eventos\n",
      "\n",
      "problema con url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 31/32 [09:52<00:19, 19.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 otras categorias\n",
      "\n",
      "problema con url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [10:07<00:00, 18.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for c_ in tqdm(df_dataset['categoria'].unique()):\n",
    "    download_images(df_dataset[df_dataset['categoria']==c_]['imagen'],c_,'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir el dataframe en train, test y val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total de registros: 15,380\n",
    "* Entrenamiento: 15,380 = 80%\n",
    "* Prueba: 1,923 = 10%\n",
    "* Validación: 1,923= 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15380, 8) (1923, 8) (1922, 8)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test = np.split(df_dataset.sample(frac=1, random_state=42),[int(.8*len(df_dataset)), int(.9*len(df_dataset))])\n",
    "print(df_train.shape, df_test.shape, df_validate.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
